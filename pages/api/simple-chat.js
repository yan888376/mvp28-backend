// Vercel AI Gateway Chat API - æŒ‰ç…§å®˜æ–¹æ ‡å‡†å®ç°
import { streamText } from 'ai';

export default async function handler(req, res) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { message, model = 'gpt-4o-mini', context = [] } = req.body;

    if (!message) {
      return res.status(400).json({ error: 'Message is required' });
    }

    // æ£€æŸ¥AI Gateway API Key
    const gatewayKey = process.env.AI_GATEWAY_API_KEY;
    if (!gatewayKey) {
      return res.status(500).json({
        error: '(æ¼”ç¤ºæ¨¡å¼)è¯·é…ç½® AI Gateway API Key',
        timestamp: new Date().toISOString()
      });
    }

    // æ„å»ºæ¶ˆæ¯ - é™åˆ¶ä¸Šä¸‹æ–‡ä»¥æé«˜å“åº”é€Ÿåº¦
    const messages = [
      ...context.slice(-2), // åªä¿ç•™æœ€è¿‘2æ¡å¯¹è¯
      { role: 'user', content: message }
    ];

    console.log(`ğŸ¤– AI Gateway Request: openai/${model}`, { 
      messageLength: message.length,
      contextLength: context.length,
      hasGatewayKey: !!gatewayKey
    });

    const startTime = Date.now();

    // ä½¿ç”¨Vercel AI Gateway - æŒ‰ç…§å®˜æ–¹æ ‡å‡†
    const result = await streamText({
      model: `openai/${model}`, // AI Gatewayæ ¼å¼
      messages: messages,
      maxTokens: 300,
      temperature: 0.7,
    });

    // è·å–å®Œæ•´æ–‡æœ¬å“åº”
    const fullText = await result.text;
    const responseTime = Date.now() - startTime;
    
    console.log(`âœ… AI Gateway Success in ${responseTime}ms`);

    return res.status(200).json({
      success: true,
      data: {
        content: fullText,
        model: `openai/${model}`,
        responseTime: `${responseTime}ms`,
        timestamp: new Date().toISOString()
      }
    });

  } catch (error) {
    console.error('âŒ AI Gateway Error:', error);
    return res.status(500).json({
      error: 'AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨',
      debug: error.message,
      timestamp: new Date().toISOString()
    });
  }
}