// Multi-GPTåŒé€šé“API - æ”¯æŒGatewayå’ŒDirectæ¨¡å¼
import { v4 as uuidv4 } from 'uuid';

// æ¨¡å‹è·¯ç”±æ³¨å†Œè¡¨
const MODEL_REGISTRY = {
  // OpenAI æ¨¡å‹
  "gpt-4o-mini": {
    provider: "openai",
    endpoint: "/chat/completions",
    gatewayPath: "/openai/v1",
    directURL: "https://api.openai.com/v1"
  },
  "gpt-4o": {
    provider: "openai", 
    endpoint: "/chat/completions",
    gatewayPath: "/openai/v1",
    directURL: "https://api.openai.com/v1"
  },
  "gpt-3.5-turbo": {
    provider: "openai",
    endpoint: "/chat/completions", 
    gatewayPath: "/openai/v1",
    directURL: "https://api.openai.com/v1"
  },
  
  // Anthropic æ¨¡å‹
  "claude-3-haiku": {
    provider: "anthropic",
    endpoint: "/messages",
    gatewayPath: "/anthropic/v1", 
    directURL: "https://api.anthropic.com/v1"
  },
  "claude-3-sonnet": {
    provider: "anthropic",
    endpoint: "/messages",
    gatewayPath: "/anthropic/v1",
    directURL: "https://api.anthropic.com/v1"
  },
  
  // Google æ¨¡å‹
  "gemini-1.5-flash": {
    provider: "google",
    endpoint: "/v1beta/models/gemini-1.5-flash:generateContent",
    gatewayPath: "/google",
    directURL: "https://generativelanguage.googleapis.com"
  },
  
  // Groq æ¨¡å‹
  "groq-llama3-70b": {
    provider: "groq", 
    endpoint: "/chat/completions",
    gatewayPath: "/groq/v1",
    directURL: "https://api.groq.com/openai/v1"
  },
  
  // Multi-GPT (H1) æ™ºèƒ½è·¯ç”± - æ˜ å°„åˆ°æœ€ä½³å¯ç”¨æ¨¡å‹
  "Multi-GPT (H1)": {
    provider: "openai",
    endpoint: "/chat/completions",
    gatewayPath: "/openai/v1", 
    directURL: "https://api.openai.com/v1"
  }
};

// æ¨¡å‹åç§°æ ‡å‡†åŒ–æ˜ å°„
const MODEL_NAME_MAPPING = {
  // OpenAI æ¨¡å‹å„ç§å†™æ³•
  'gpt-4o-mini': 'gpt-4o-mini',
  'GPT-4o Mini': 'gpt-4o-mini',
  'GPT-4o-mini': 'gpt-4o-mini',
  'gpt4o-mini': 'gpt-4o-mini',
  
  'gpt-4o': 'gpt-4o',
  'GPT-4o': 'gpt-4o',
  'GPT4o': 'gpt-4o',
  'gpt4o': 'gpt-4o',
  
  'gpt-3.5-turbo': 'gpt-3.5-turbo',
  'GPT-3.5 Turbo': 'gpt-3.5-turbo',
  'GPT-3.5-Turbo': 'gpt-3.5-turbo',
  'gpt35turbo': 'gpt-3.5-turbo',
  'gpt-35-turbo': 'gpt-3.5-turbo',
  
  // Anthropic æ¨¡å‹
  'claude-3-haiku': 'claude-3-haiku',
  'Claude 3 Haiku': 'claude-3-haiku',
  'Claude-3-Haiku': 'claude-3-haiku',
  
  'claude-3-sonnet': 'claude-3-sonnet',
  'Claude 3 Sonnet': 'claude-3-sonnet',
  'Claude-3-Sonnet': 'claude-3-sonnet',
  
  // Google æ¨¡å‹
  'gemini-1.5-flash': 'gemini-1.5-flash',
  'Gemini 1.5 Flash': 'gemini-1.5-flash',
  'Gemini-1.5-Flash': 'gemini-1.5-flash',
  
  // Groq æ¨¡å‹
  'groq-llama3-70b': 'groq-llama3-70b',
  'Groq Llama3 70B': 'groq-llama3-70b',
  
  // Multi-GPT (H1) æ˜ å°„
  'Multi-GPT (H1)': 'Multi-GPT (H1)',
  'multi-gpt-h1': 'Multi-GPT (H1)',
  'Multi-GPT H1': 'Multi-GPT (H1)'
};

// æ ‡å‡†åŒ–æ¨¡å‹åç§°
function normalizeModelName(modelName) {
  if (!modelName) return 'gpt-4o-mini'; // é»˜è®¤æ¨¡å‹
  
  // ç›´æ¥åŒ¹é…
  if (MODEL_NAME_MAPPING[modelName]) {
    return MODEL_NAME_MAPPING[modelName];
  }
  
  // å¿½ç•¥å¤§å°å†™åŒ¹é…
  const lowerModel = modelName.toLowerCase();
  for (const [key, value] of Object.entries(MODEL_NAME_MAPPING)) {
    if (key.toLowerCase() === lowerModel) {
      return value;
    }
  }
  
  // æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤ç©ºæ ¼ã€è¿å­—ç¬¦ã€ç‚¹å·ï¼‰
  const normalizedInput = modelName.toLowerCase().replace(/[\s\-\.]/g, '');
  for (const [key, value] of Object.entries(MODEL_NAME_MAPPING)) {
    const normalizedKey = key.toLowerCase().replace(/[\s\-\.]/g, '');
    if (normalizedKey === normalizedInput) {
      return value;
    }
  }
  
  // å¦‚æœéƒ½åŒ¹é…ä¸ä¸Šï¼Œè¿”å›åŸå€¼ï¼ˆè®©åç»­æŠ¥é”™ï¼‰
  return modelName;
}

// è·å–APIé…ç½®
function getAPIConfig(model) {
  const modelConfig = MODEL_REGISTRY[model];
  if (!modelConfig) {
    throw new Error(`Unsupported model: ${model}`);
  }
  
  const useGateway = process.env.USE_GATEWAY === 'true';
  const provider = modelConfig.provider;
  
  let config = {
    provider,
    model,
    useGateway,
    baseURL: useGateway ? 
      `${process.env.AI_GATEWAY_URL}${modelConfig.gatewayPath}` : 
      modelConfig.directURL,
    endpoint: modelConfig.endpoint,
    headers: {
      'Content-Type': 'application/json'
    }
  };
  
  // è®¾ç½®è®¤è¯
  if (useGateway) {
    if (!process.env.AI_GATEWAY_TOKEN) {
      throw new Error('AI_GATEWAY_TOKEN not configured');
    }
    config.headers['Authorization'] = `Bearer ${process.env.AI_GATEWAY_TOKEN}`;
  } else {
    const apiKeyMap = {
      openai: process.env.OPENAI_API_KEY,
      anthropic: process.env.ANTHROPIC_API_KEY,
      google: process.env.GOOGLE_API_KEY,
      groq: process.env.GROQ_API_KEY,
      cohere: process.env.COHERE_API_KEY,
      openrouter: process.env.OPENROUTER_API_KEY
    };
    
    const apiKey = apiKeyMap[provider];
    if (!apiKey) {
      throw new Error(`${provider.toUpperCase()}_API_KEY not configured`);
    }
    
    // ä¸åŒä¾›åº”å•†çš„è®¤è¯æ ¼å¼
    if (provider === 'anthropic') {
      config.headers['x-api-key'] = apiKey;
      config.headers['anthropic-version'] = '2023-06-01';
    } else if (provider === 'google') {
      config.headers['x-goog-api-key'] = apiKey;
    } else {
      config.headers['Authorization'] = `Bearer ${apiKey}`;
    }
  }
  
  return config;
}

// è·å–ä¸Šæ¸¸æ¨¡å‹åç§°
function getUpstreamModel(model, provider) {
  // Multi-GPT (H1) æ˜ å°„åˆ°æœ€ä½³å¯ç”¨æ¨¡å‹
  if (model === 'Multi-GPT (H1)') {
    return 'gpt-4o-mini'; // é»˜è®¤ä½¿ç”¨æœ€ç¨³å®šçš„æ¨¡å‹
  }
  
  // å…¶ä»–æ¨¡å‹ç›´æ¥ä½¿ç”¨æ ‡å‡†åŒ–åçš„åç§°
  return model;
}

// æ„å»ºè¯·æ±‚ä½“
function buildRequestBody(messages, model, provider) {
  const upstreamModel = getUpstreamModel(model, provider);
  
  const baseBody = {
    model: upstreamModel,
    max_tokens: 1000,
    temperature: 0.7
  };
  
  if (provider === 'anthropic') {
    // Anthropicæ ¼å¼
    return {
      ...baseBody,
      messages: messages.map(msg => ({
        role: msg.role,
        content: msg.content
      }))
    };
  } else if (provider === 'google') {
    // Googleæ ¼å¼
    return {
      contents: messages.map(msg => ({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }]
      })),
      generationConfig: {
        maxOutputTokens: 1000,
        temperature: 0.7
      }
    };
  } else {
    // OpenAI/Groqæ ‡å‡†æ ¼å¼
    return {
      ...baseBody,
      messages: messages
    };
  }
}

// è§£æå“åº”
function parseResponse(data, provider) {
  if (provider === 'anthropic') {
    return data.content?.[0]?.text || '';
  } else if (provider === 'google') {
    return data.candidates?.[0]?.content?.parts?.[0]?.text || '';
  } else {
    // OpenAI/Groqæ ‡å‡†æ ¼å¼
    return data.choices?.[0]?.message?.content || '';
  }
}

export default async function handler(req, res) {
  const traceId = uuidv4();
  const startTime = Date.now();
  
  if (req.method !== 'POST') {
    return res.status(405).json({ 
      error: 'METHOD_NOT_ALLOWED',
      traceId 
    });
  }

  // ç¯å¢ƒå˜é‡æ£€æŸ¥å’Œè°ƒè¯•æ—¥å¿—
  const gatewayMode = process.env.USE_GATEWAY === 'true';
  const hasGatewayToken = !!process.env.AI_GATEWAY_TOKEN;
  const hasOpenAIKey = !!process.env.OPENAI_API_KEY;
  
  console.log(`ğŸ”§ [${traceId}] Environment check:`, {
    gatewayMode,
    hasGatewayToken,
    hasOpenAIKey,
    openaiKeyLength: process.env.OPENAI_API_KEY?.length || 0,
    gatewayTokenPrefix: process.env.AI_GATEWAY_TOKEN?.slice(0, 6) || 'none'
  });
  
  // æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
  if (gatewayMode && !hasGatewayToken) {
    return res.status(400).json({
      error: 'CONFIGURATION_ERROR',
      detail: 'AI_GATEWAY_TOKEN not configured for gateway mode',
      traceId
    });
  }
  
  if (!gatewayMode && !hasOpenAIKey) {
    return res.status(400).json({
      error: 'CONFIGURATION_ERROR', 
      detail: 'OPENAI_API_KEY not configured for direct mode',
      traceId
    });
  }

  try {
    const { message, messages, model = 'gpt-4o-mini', context = [] } = req.body;
    
    // æ ‡å‡†åŒ–æ¨¡å‹åç§°
    const normalizedModel = normalizeModelName(model);
    
    console.log(`ğŸ”„ [${traceId}] Model normalization: "${model}" â†’ "${normalizedModel}"`);
    
    // å…¼å®¹ä¸¤ç§è¯·æ±‚æ ¼å¼
    let finalMessages = [];
    if (messages) {
      finalMessages = messages;
    } else if (message) {
      finalMessages = [
        ...context.slice(-3), // é™åˆ¶ä¸Šä¸‹æ–‡
        { role: 'user', content: message }
      ];
    } else {
      return res.status(400).json({
        error: 'MISSING_MESSAGE',
        detail: 'Either "message" or "messages" field is required',
        traceId
      });
    }

    console.log(`ğŸš€ [${traceId}] Multi-chat request: model=${normalizedModel}, messages=${finalMessages.length}`);

    // è·å–APIé…ç½®
    const config = getAPIConfig(normalizedModel);
    const fullURL = `${config.baseURL}${config.endpoint}`;
    
    console.log(`ğŸ¯ [${traceId}] Config: provider=${config.provider}, mode=${config.useGateway ? 'gateway' : 'direct'}, url=${fullURL}`);

    // æ„å»ºè¯·æ±‚ä½“
    const requestBody = buildRequestBody(finalMessages, normalizedModel, config.provider);
    
    // è°ƒç”¨ä¸Šæ¸¸API
    const response = await fetch(fullURL, {
      method: 'POST',
      headers: config.headers,
      body: JSON.stringify(requestBody),
      timeout: 30000
    });

    const responseData = await response.json();
    const latency = Date.now() - startTime;

    console.log(`ğŸ“Š [${traceId}] Response: status=${response.status}, latency=${latency}ms`);

    if (!response.ok) {
      console.error(`âŒ [${traceId}] Upstream error:`, {
        status: response.status,
        provider: config.provider,
        mode: config.useGateway ? 'gateway' : 'direct',
        error: responseData
      });
      
      return res.status(response.status >= 500 ? 502 : response.status).json({
        error: 'UPSTREAM_ERROR',
        provider: config.provider,
        mode: config.useGateway ? 'gateway' : 'direct',
        status: response.status,
        detail: responseData.error?.message || 'Upstream service error',
        traceId
      });
    }

    // è§£æå“åº”å†…å®¹
    const content = parseResponse(responseData, config.provider);
    
    if (!content) {
      console.error(`âŒ [${traceId}] Empty response from ${config.provider}`);
      return res.status(502).json({
        error: 'EMPTY_RESPONSE',
        provider: config.provider,
        detail: 'No content in upstream response',
        traceId
      });
    }

    console.log(`âœ… [${traceId}] Success: ${content.length} chars, ${latency}ms`);

    return res.status(200).json({
      success: true,
      data: {
        content,
        model: normalizedModel,
        provider: config.provider,
        mode: config.useGateway ? 'gateway' : 'direct',
        latency: `${latency}ms`,
        traceId
      }
    });

  } catch (error) {
    const latency = Date.now() - startTime;
    console.error(`ğŸ’¥ [${traceId}] Error: ${error.message}`);
    
    return res.status(500).json({
      error: 'INTERNAL_ERROR',
      detail: error.message,
      latency: `${latency}ms`,
      traceId
    });
  }
}