// AI Model Provider abstraction layer
import OpenAI from 'openai'
import { config } from '../config'

export interface ChatMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

export interface ChatResponse {
  content: string
  model: string
  latencyMs: number
  tokenUsage?: {
    promptTokens: number
    completionTokens: number
    totalTokens: number
  }
}

export abstract class ModelProvider {
  abstract chat(messages: ChatMessage[], model?: string): Promise<ChatResponse>
  abstract chatStream?(messages: ChatMessage[], model?: string): AsyncGenerator<string>
}

export class OpenAIProvider extends ModelProvider {
  private client: OpenAI

  constructor() {
    super()
    this.client = new OpenAI({
      apiKey: config.ai.openaiApiKey,
      timeout: config.ai.timeoutMs,
    })
  }

  async chat(messages: ChatMessage[], model = 'gpt-4o-mini'): Promise<ChatResponse> {
    const startTime = Date.now()
    
    // éªŒè¯ API Key
    if (!config.ai.openaiApiKey || config.ai.openaiApiKey === 'sk-your-openai-api-key-here') {
      throw new Error('OpenAI API key not configured')
    }

    // ä¼˜åŒ–ä¸Šä¸‹æ–‡ç®¡ç†ï¼šé™åˆ¶æ¶ˆæ¯æ•°é‡å’Œæ€»tokenæ•°
    const optimizedMessages = this.optimizeMessageContext(messages)
    
    console.log(`ğŸ¤– è°ƒç”¨ OpenAI API: model=${model}, messages=${optimizedMessages.length}æ¡ (åŸå§‹${messages.length}æ¡)`)
    
    // å®ç°é‡è¯•æœºåˆ¶
    return await this.executeWithRetry(async () => {
      const response = await this.client.chat.completions.create({
        model,
        messages: optimizedMessages,
        max_tokens: this.calculateMaxTokens(optimizedMessages, model),
        temperature: config.ai.temperature,
        // æ·»åŠ é¢å¤–çš„å®‰å…¨å‚æ•°
        presence_penalty: 0.1,
        frequency_penalty: 0.1,
        // é˜²æ­¢æœ‰å®³å†…å®¹
        moderation: true,
      })

      const latencyMs = Date.now() - startTime
      const choice = response.choices[0]
      
      if (!choice?.message?.content) {
        throw new Error('No content in OpenAI response')
      }

      // æ£€æŸ¥å†…å®¹è´¨é‡
      if (choice.message.content.trim().length < 2) {
        throw new Error('OpenAI response too short')
      }

      console.log(`âœ… OpenAI API æˆåŠŸ: ${latencyMs}ms, tokens=${response.usage?.total_tokens}`)

      return {
        content: choice.message.content,
        model: response.model,
        latencyMs,
        tokenUsage: response.usage ? {
          promptTokens: response.usage.prompt_tokens,
          completionTokens: response.usage.completion_tokens,
          totalTokens: response.usage.total_tokens,
        } : undefined,
      }
    }, model, startTime)
  }

  // æ™ºèƒ½ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼šä¿ç•™æœ€é‡è¦çš„æ¶ˆæ¯
  private optimizeMessageContext(messages: ChatMessage[]): ChatMessage[] {
    const maxMessages = 20 // æœ€å¤§æ¶ˆæ¯æ•°
    const maxContextTokens = 3000 // å¤§çº¦çš„tokené™åˆ¶
    
    if (messages.length <= maxMessages) {
      return messages
    }

    // ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„å¯¹è¯
    const systemMessages = messages.filter(m => m.role === 'system')
    const conversationMessages = messages.filter(m => m.role !== 'system')
    
    // ä¿ç•™æœ€è¿‘çš„å¯¹è¯
    const recentMessages = conversationMessages.slice(-maxMessages + systemMessages.length)
    
    return [...systemMessages, ...recentMessages]
  }

  // åŠ¨æ€è®¡ç®—æœ€å¤§tokenæ•°
  private calculateMaxTokens(messages: ChatMessage[], model: string): number {
    // ä¼°ç®—è¾“å…¥tokenæ•° (ç²—ç•¥ä¼°ç®—: 1ä¸ªè‹±æ–‡è¯â‰ˆ1.3ä¸ªtokenï¼Œ1ä¸ªä¸­æ–‡å­—â‰ˆ2ä¸ªtoken)
    const estimatedInputTokens = messages.reduce((total, msg) => {
      const words = msg.content.split(/\s+/).length
      const chineseChars = (msg.content.match(/[\u4e00-\u9fff]/g) || []).length
      return total + words * 1.3 + chineseChars * 2
    }, 0)

    // æ ¹æ®æ¨¡å‹è°ƒæ•´tokené™åˆ¶
    const modelLimits = {
      'gpt-4o-mini': 128000,
      'gpt-4o': 128000,
      'gpt-4': 8192,
      'gpt-3.5-turbo': 4096
    }

    const maxModelTokens = modelLimits[model as keyof typeof modelLimits] || 4096
    const maxOutputTokens = Math.min(
      config.ai.maxTokens,
      Math.max(500, maxModelTokens - estimatedInputTokens - 500) // ä¿ç•™500 tokenç¼“å†²
    )

    return maxOutputTokens
  }

  // é‡è¯•æœºåˆ¶å®ç°
  private async executeWithRetry<T>(
    operation: () => Promise<T>,
    model: string,
    startTime: number,
    maxRetries: number = 3
  ): Promise<T> {
    let lastError: any
    
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        return await operation()
      } catch (error: any) {
        lastError = error
        
        console.log(`âŒ OpenAI API å°è¯• ${attempt}/${maxRetries} å¤±è´¥:`, {
          message: error.message,
          status: error.status,
          code: error.code,
          attempt
        })
        
        // åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
        if (!this.shouldRetry(error, attempt, maxRetries)) {
          break
        }
        
        // æŒ‡æ•°é€€é¿å»¶è¿Ÿ
        const delay = Math.min(1000 * Math.pow(2, attempt - 1), 10000)
        console.log(`â³ ${delay}ms åé‡è¯•...`)
        await new Promise(resolve => setTimeout(resolve, delay))
      }
    }
    
    // æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é™çº§å“åº”
    const latencyMs = Date.now() - startTime
    return this.createFallbackResponse(lastError, latencyMs) as T
  }

  // åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
  private shouldRetry(error: any, attempt: number, maxRetries: number): boolean {
    if (attempt >= maxRetries) return false
    
    // è¿™äº›é”™è¯¯ä¸åº”è¯¥é‡è¯•
    const nonRetryableErrors = [401, 402, 403] // è®¤è¯ã€è´¦å•ã€æƒé™é”™è¯¯
    if (nonRetryableErrors.includes(error.status)) {
      return false
    }
    
    // è¿™äº›é”™è¯¯å¯ä»¥é‡è¯•
    const retryableErrors = [429, 500, 502, 503, 504] // é¢‘ç‡é™åˆ¶ã€æœåŠ¡å™¨é”™è¯¯
    const networkErrors = ['ENOTFOUND', 'ECONNREFUSED', 'ETIMEDOUT']
    
    return retryableErrors.includes(error.status) || 
           networkErrors.includes(error.code) ||
           error.message?.includes('timeout')
  }

  // åˆ›å»ºé™çº§å“åº”
  private createFallbackResponse(error: any, latencyMs: number): ChatResponse {
    // è¯¦ç»†é”™è¯¯æ—¥å¿—
    console.error('âŒ OpenAI API æœ€ç»ˆå¤±è´¥:', {
      message: error.message,
      status: error.status,
      code: error.code,
      type: error.type,
      latencyMs
    })
    
    // ä¸åŒé”™è¯¯ç±»å‹çš„å¤„ç†
    let fallbackMessage = 'æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚'
    
    if (error.status === 401) {
      fallbackMessage = 'âŒ APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®'
      console.error('ğŸ”‘ è¯·æ£€æŸ¥ OPENAI_API_KEY ç¯å¢ƒå˜é‡é…ç½®')
    } else if (error.status === 429) {
      fallbackMessage = 'âš ï¸ APIè°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•'
    } else if (error.status === 402) {
      fallbackMessage = 'ğŸ’³ OpenAIè´¦æˆ·ä½™é¢ä¸è¶³ï¼Œè¯·å……å€¼'
    } else if (error.code === 'ENOTFOUND' || error.code === 'ECONNREFUSED') {
      fallbackMessage = 'ğŸŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®'
    } else if (error.message?.includes('timeout')) {
      fallbackMessage = 'â±ï¸ APIå“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
    }
    
    return {
      content: fallbackMessage,
      model: 'fallback',
      latencyMs,
    }
  }

  // Future: Stream implementation
  async *chatStream(messages: ChatMessage[], model = 'gpt-4o-mini'): AsyncGenerator<string> {
    const stream = await this.client.chat.completions.create({
      model,
      messages,
      max_tokens: config.ai.maxTokens,
      temperature: config.ai.temperature,
      stream: true,
    })

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content
      if (content) {
        yield content
      }
    }
  }
}

// Provider factory
export function createModelProvider(): ModelProvider {
  switch (config.ai.provider) {
    case 'openai':
      return new OpenAIProvider()
    default:
      throw new Error(`Unsupported AI provider: ${config.ai.provider}`)
  }
}